{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "High-Dim fractal.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTrNV4v6KBJP"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras import backend\n",
        "import pandas as pd\n",
        "import io\n",
        "import os\n",
        "import requests\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import math\n",
        "from math import log\n",
        "import numpy as np \n",
        "import matplotlib \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "import itertools\n",
        "from itertools import product"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate Cantor Sets"
      ],
      "metadata": {
        "id": "cL0ESTzAKJCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x=[]\n",
        "y=[]\n",
        "nvalue=4 \n",
        "\n",
        "def xinA(list,n,k,power):  # for the case x in A\n",
        "    for an in [0, 1]:\n",
        "        k = k + an*(3**(-power))\n",
        "\n",
        "        if power < n:\n",
        "            xinA(x, n, k, power + 1)\n",
        "\n",
        "        else :\n",
        "            x.append(k)\n",
        "\n",
        "\n",
        "def xinB(list,n,k,power):  # for the case x in B\n",
        "    for an in [0, 2]:\n",
        "        k = k + an * (3 ** (-power))\n",
        "\n",
        "        if power < n:\n",
        "            xinB(y, n, k, power + 1)\n",
        "\n",
        "        else:\n",
        "            y.append(k)\n",
        "\n",
        "xinA(x,nvalue,0,1)\n",
        "\n",
        "\n",
        "xinB(y,nvalue,0,1)\n",
        "\n",
        "\n",
        "cartesian = list(product(x, y))  # cartesian product to make the set becomes 2D\n",
        "\n",
        "\n",
        "print(cartesian)\n",
        "\n",
        "\n",
        "listoflist = np.array(cartesian)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7o2BinWKQrf",
        "outputId": "e717fe0e-ba81-4ddf-cc6b-af2188868908"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0.0, 0.0), (0.0, 0.024691358024691357), (0.0, 0.07407407407407407), (0.0, 0.09876543209876543), (0.0, 0.2222222222222222), (0.0, 0.24691358024691357), (0.0, 0.2962962962962963), (0.0, 0.32098765432098764), (0.0, 0.6666666666666666), (0.0, 0.691358024691358), (0.0, 0.7407407407407407), (0.0, 0.7654320987654321), (0.0, 0.8888888888888888), (0.0, 0.9135802469135802), (0.0, 0.9629629629629629), (0.0, 0.9876543209876543), (0.012345679012345678, 0.0), (0.012345679012345678, 0.024691358024691357), (0.012345679012345678, 0.07407407407407407), (0.012345679012345678, 0.09876543209876543), (0.012345679012345678, 0.2222222222222222), (0.012345679012345678, 0.24691358024691357), (0.012345679012345678, 0.2962962962962963), (0.012345679012345678, 0.32098765432098764), (0.012345679012345678, 0.6666666666666666), (0.012345679012345678, 0.691358024691358), (0.012345679012345678, 0.7407407407407407), (0.012345679012345678, 0.7654320987654321), (0.012345679012345678, 0.8888888888888888), (0.012345679012345678, 0.9135802469135802), (0.012345679012345678, 0.9629629629629629), (0.012345679012345678, 0.9876543209876543), (0.037037037037037035, 0.0), (0.037037037037037035, 0.024691358024691357), (0.037037037037037035, 0.07407407407407407), (0.037037037037037035, 0.09876543209876543), (0.037037037037037035, 0.2222222222222222), (0.037037037037037035, 0.24691358024691357), (0.037037037037037035, 0.2962962962962963), (0.037037037037037035, 0.32098765432098764), (0.037037037037037035, 0.6666666666666666), (0.037037037037037035, 0.691358024691358), (0.037037037037037035, 0.7407407407407407), (0.037037037037037035, 0.7654320987654321), (0.037037037037037035, 0.8888888888888888), (0.037037037037037035, 0.9135802469135802), (0.037037037037037035, 0.9629629629629629), (0.037037037037037035, 0.9876543209876543), (0.04938271604938271, 0.0), (0.04938271604938271, 0.024691358024691357), (0.04938271604938271, 0.07407407407407407), (0.04938271604938271, 0.09876543209876543), (0.04938271604938271, 0.2222222222222222), (0.04938271604938271, 0.24691358024691357), (0.04938271604938271, 0.2962962962962963), (0.04938271604938271, 0.32098765432098764), (0.04938271604938271, 0.6666666666666666), (0.04938271604938271, 0.691358024691358), (0.04938271604938271, 0.7407407407407407), (0.04938271604938271, 0.7654320987654321), (0.04938271604938271, 0.8888888888888888), (0.04938271604938271, 0.9135802469135802), (0.04938271604938271, 0.9629629629629629), (0.04938271604938271, 0.9876543209876543), (0.1111111111111111, 0.0), (0.1111111111111111, 0.024691358024691357), (0.1111111111111111, 0.07407407407407407), (0.1111111111111111, 0.09876543209876543), (0.1111111111111111, 0.2222222222222222), (0.1111111111111111, 0.24691358024691357), (0.1111111111111111, 0.2962962962962963), (0.1111111111111111, 0.32098765432098764), (0.1111111111111111, 0.6666666666666666), (0.1111111111111111, 0.691358024691358), (0.1111111111111111, 0.7407407407407407), (0.1111111111111111, 0.7654320987654321), (0.1111111111111111, 0.8888888888888888), (0.1111111111111111, 0.9135802469135802), (0.1111111111111111, 0.9629629629629629), (0.1111111111111111, 0.9876543209876543), (0.12345679012345678, 0.0), (0.12345679012345678, 0.024691358024691357), (0.12345679012345678, 0.07407407407407407), (0.12345679012345678, 0.09876543209876543), (0.12345679012345678, 0.2222222222222222), (0.12345679012345678, 0.24691358024691357), (0.12345679012345678, 0.2962962962962963), (0.12345679012345678, 0.32098765432098764), (0.12345679012345678, 0.6666666666666666), (0.12345679012345678, 0.691358024691358), (0.12345679012345678, 0.7407407407407407), (0.12345679012345678, 0.7654320987654321), (0.12345679012345678, 0.8888888888888888), (0.12345679012345678, 0.9135802469135802), (0.12345679012345678, 0.9629629629629629), (0.12345679012345678, 0.9876543209876543), (0.14814814814814814, 0.0), (0.14814814814814814, 0.024691358024691357), (0.14814814814814814, 0.07407407407407407), (0.14814814814814814, 0.09876543209876543), (0.14814814814814814, 0.2222222222222222), (0.14814814814814814, 0.24691358024691357), (0.14814814814814814, 0.2962962962962963), (0.14814814814814814, 0.32098765432098764), (0.14814814814814814, 0.6666666666666666), (0.14814814814814814, 0.691358024691358), (0.14814814814814814, 0.7407407407407407), (0.14814814814814814, 0.7654320987654321), (0.14814814814814814, 0.8888888888888888), (0.14814814814814814, 0.9135802469135802), (0.14814814814814814, 0.9629629629629629), (0.14814814814814814, 0.9876543209876543), (0.16049382716049382, 0.0), (0.16049382716049382, 0.024691358024691357), (0.16049382716049382, 0.07407407407407407), (0.16049382716049382, 0.09876543209876543), (0.16049382716049382, 0.2222222222222222), (0.16049382716049382, 0.24691358024691357), (0.16049382716049382, 0.2962962962962963), (0.16049382716049382, 0.32098765432098764), (0.16049382716049382, 0.6666666666666666), (0.16049382716049382, 0.691358024691358), (0.16049382716049382, 0.7407407407407407), (0.16049382716049382, 0.7654320987654321), (0.16049382716049382, 0.8888888888888888), (0.16049382716049382, 0.9135802469135802), (0.16049382716049382, 0.9629629629629629), (0.16049382716049382, 0.9876543209876543), (0.3333333333333333, 0.0), (0.3333333333333333, 0.024691358024691357), (0.3333333333333333, 0.07407407407407407), (0.3333333333333333, 0.09876543209876543), (0.3333333333333333, 0.2222222222222222), (0.3333333333333333, 0.24691358024691357), (0.3333333333333333, 0.2962962962962963), (0.3333333333333333, 0.32098765432098764), (0.3333333333333333, 0.6666666666666666), (0.3333333333333333, 0.691358024691358), (0.3333333333333333, 0.7407407407407407), (0.3333333333333333, 0.7654320987654321), (0.3333333333333333, 0.8888888888888888), (0.3333333333333333, 0.9135802469135802), (0.3333333333333333, 0.9629629629629629), (0.3333333333333333, 0.9876543209876543), (0.345679012345679, 0.0), (0.345679012345679, 0.024691358024691357), (0.345679012345679, 0.07407407407407407), (0.345679012345679, 0.09876543209876543), (0.345679012345679, 0.2222222222222222), (0.345679012345679, 0.24691358024691357), (0.345679012345679, 0.2962962962962963), (0.345679012345679, 0.32098765432098764), (0.345679012345679, 0.6666666666666666), (0.345679012345679, 0.691358024691358), (0.345679012345679, 0.7407407407407407), (0.345679012345679, 0.7654320987654321), (0.345679012345679, 0.8888888888888888), (0.345679012345679, 0.9135802469135802), (0.345679012345679, 0.9629629629629629), (0.345679012345679, 0.9876543209876543), (0.37037037037037035, 0.0), (0.37037037037037035, 0.024691358024691357), (0.37037037037037035, 0.07407407407407407), (0.37037037037037035, 0.09876543209876543), (0.37037037037037035, 0.2222222222222222), (0.37037037037037035, 0.24691358024691357), (0.37037037037037035, 0.2962962962962963), (0.37037037037037035, 0.32098765432098764), (0.37037037037037035, 0.6666666666666666), (0.37037037037037035, 0.691358024691358), (0.37037037037037035, 0.7407407407407407), (0.37037037037037035, 0.7654320987654321), (0.37037037037037035, 0.8888888888888888), (0.37037037037037035, 0.9135802469135802), (0.37037037037037035, 0.9629629629629629), (0.37037037037037035, 0.9876543209876543), (0.38271604938271603, 0.0), (0.38271604938271603, 0.024691358024691357), (0.38271604938271603, 0.07407407407407407), (0.38271604938271603, 0.09876543209876543), (0.38271604938271603, 0.2222222222222222), (0.38271604938271603, 0.24691358024691357), (0.38271604938271603, 0.2962962962962963), (0.38271604938271603, 0.32098765432098764), (0.38271604938271603, 0.6666666666666666), (0.38271604938271603, 0.691358024691358), (0.38271604938271603, 0.7407407407407407), (0.38271604938271603, 0.7654320987654321), (0.38271604938271603, 0.8888888888888888), (0.38271604938271603, 0.9135802469135802), (0.38271604938271603, 0.9629629629629629), (0.38271604938271603, 0.9876543209876543), (0.4444444444444444, 0.0), (0.4444444444444444, 0.024691358024691357), (0.4444444444444444, 0.07407407407407407), (0.4444444444444444, 0.09876543209876543), (0.4444444444444444, 0.2222222222222222), (0.4444444444444444, 0.24691358024691357), (0.4444444444444444, 0.2962962962962963), (0.4444444444444444, 0.32098765432098764), (0.4444444444444444, 0.6666666666666666), (0.4444444444444444, 0.691358024691358), (0.4444444444444444, 0.7407407407407407), (0.4444444444444444, 0.7654320987654321), (0.4444444444444444, 0.8888888888888888), (0.4444444444444444, 0.9135802469135802), (0.4444444444444444, 0.9629629629629629), (0.4444444444444444, 0.9876543209876543), (0.4567901234567901, 0.0), (0.4567901234567901, 0.024691358024691357), (0.4567901234567901, 0.07407407407407407), (0.4567901234567901, 0.09876543209876543), (0.4567901234567901, 0.2222222222222222), (0.4567901234567901, 0.24691358024691357), (0.4567901234567901, 0.2962962962962963), (0.4567901234567901, 0.32098765432098764), (0.4567901234567901, 0.6666666666666666), (0.4567901234567901, 0.691358024691358), (0.4567901234567901, 0.7407407407407407), (0.4567901234567901, 0.7654320987654321), (0.4567901234567901, 0.8888888888888888), (0.4567901234567901, 0.9135802469135802), (0.4567901234567901, 0.9629629629629629), (0.4567901234567901, 0.9876543209876543), (0.48148148148148145, 0.0), (0.48148148148148145, 0.024691358024691357), (0.48148148148148145, 0.07407407407407407), (0.48148148148148145, 0.09876543209876543), (0.48148148148148145, 0.2222222222222222), (0.48148148148148145, 0.24691358024691357), (0.48148148148148145, 0.2962962962962963), (0.48148148148148145, 0.32098765432098764), (0.48148148148148145, 0.6666666666666666), (0.48148148148148145, 0.691358024691358), (0.48148148148148145, 0.7407407407407407), (0.48148148148148145, 0.7654320987654321), (0.48148148148148145, 0.8888888888888888), (0.48148148148148145, 0.9135802469135802), (0.48148148148148145, 0.9629629629629629), (0.48148148148148145, 0.9876543209876543), (0.49382716049382713, 0.0), (0.49382716049382713, 0.024691358024691357), (0.49382716049382713, 0.07407407407407407), (0.49382716049382713, 0.09876543209876543), (0.49382716049382713, 0.2222222222222222), (0.49382716049382713, 0.24691358024691357), (0.49382716049382713, 0.2962962962962963), (0.49382716049382713, 0.32098765432098764), (0.49382716049382713, 0.6666666666666666), (0.49382716049382713, 0.691358024691358), (0.49382716049382713, 0.7407407407407407), (0.49382716049382713, 0.7654320987654321), (0.49382716049382713, 0.8888888888888888), (0.49382716049382713, 0.9135802469135802), (0.49382716049382713, 0.9629629629629629), (0.49382716049382713, 0.9876543209876543)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xaxis = []\n",
        "\n",
        "def CheckDuplicate(listSE, input):\n",
        "    for element in listSE:\n",
        "        if input == element:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "\n",
        "# the main algorithm to find those points('s coordinate) in Cantor set, I start it from the one dimension case\n",
        "# and do the cartesian product to get the 2 dimension Cantor set\n",
        "def recursiveCantor(listSE, Start, End, n):\n",
        "    len = End - Start\n",
        "    FirstQ = [Start, Start + len / 3]\n",
        "    ForthQ = [End - len / 3, End]\n",
        "    # print(ForthQ)\n",
        "    if n >= 1:\n",
        "        recursiveCantor(FirstQ, Start, Start + len / 3, n - 1)\n",
        "        recursiveCantor(ForthQ, End - len / 3, End, n - 1)\n",
        "    if CheckDuplicate(xaxis, Start) == True:\n",
        "        xaxis.append(Start)\n",
        "    if CheckDuplicate(xaxis, End) == True:\n",
        "        xaxis.append(End)\n",
        "\n",
        "recursiveCantor([0, 1], 0, 1, 3)   # iterate for 4 times,\n",
        "\n",
        "print(xaxis)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDd7i9RBGzui",
        "outputId": "66c7a02c-a888-4baf-e8aa-02d81c42af56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0.037037037037037035, 0.07407407407407407, 0.1111111111111111, 0.2222222222222222, 0.25925925925925924, 0.2962962962962963, 0.3333333333333333, 0.6666666666666667, 0.7037037037037037, 0.7407407407407408, 0.7777777777777778, 0.888888888888889, 0.9259259259259259, 0.962962962962963, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the second method for testing"
      ],
      "metadata": {
        "id": "31G_YmewG0nZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Distencefunction(HighDimVar):\n",
        "  HighDimVar=np.array(HighDimVar)\n",
        "  sum=0\n",
        "  #i,j = product\n",
        "  #return math.sqrt(i*i+j*j)\n",
        "  for i in range(len(HighDimVar)):\n",
        "    sum = sum + HighDimVar[i]**2\n",
        "  return math.sqrt(sum)\n",
        "\n",
        "oneDimlist=[]\n",
        "\n",
        "twoDimlist=[]\n",
        "\n",
        "outputlist=[]\n",
        "\n",
        "def HighDim(CarProdut,NumofCarProdut):  #NumofCarProdut, it makes sense for training purpose for a value larger than 2\n",
        "\n",
        "  CarProdut=list(itertools.product(CarProdut, repeat=NumofCarProdut))\n",
        "\n",
        "  for i in range(len(CarProdut)):\n",
        "      twoDimlist.append(CarProdut[i])\n",
        "      #print(CarProdut)\n",
        "\n",
        "\n",
        "  for i in range(len(CarProdut)):\n",
        "        #print(sum(CarProdut[i]))\n",
        "        oneDimlist.append(sum(CarProdut[i]))\n",
        "  \n",
        "  for i in range(len(twoDimlist)):\n",
        "      outputlist.append(Distencefunction(twoDimlist[i]))\n",
        "\n",
        "HighDim(xaxis,2)\n",
        "\n",
        "\n",
        "\n",
        "print(outputlist)\n",
        "\n",
        "print(len(oneDimlist))\n",
        "print(len(twoDimlist))\n",
        "\n",
        "print(twoDimlist[4])\n",
        "print(Distencefunction(twoDimlist[4]))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjOxksrlNUtS",
        "outputId": "74fc4047-10b2-415e-c2e1-06bf223fbc69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.0, 0.037037037037037035, 0.07407407407407407, 0.1111111111111111, 0.2222222222222222, 0.25925925925925924, 0.2962962962962963, 0.3333333333333333, 0.6666666666666667, 0.7037037037037037, 0.7407407407407408, 0.7777777777777778, 0.888888888888889, 0.9259259259259259, 0.962962962962963, 1.0, 0.037037037037037035, 0.05237828008789241, 0.08281733249999221, 0.11712139482105108, 0.2252875011221563, 0.261891400439462, 0.2986021388258722, 0.33538463474583025, 0.6676946806414795, 0.7046776885348315, 0.7416660886852143, 0.7786591126529125, 0.8896601592195789, 0.9266663706071706, 0.9636749504635325, 1.0006856360078737, 0.07407407407407407, 0.08281733249999221, 0.10475656017578482, 0.1335389361282959, 0.23424278964210216, 0.2696336996029821, 0.3054152315272341, 0.3414646095293662, 0.6707692694916606, 0.7075915990571409, 0.7444352311941401, 0.7812971522121848, 0.8919699687994294, 0.9288841632581076, 0.9658077637337258, 1.002739731161547, 0.1111111111111111, 0.11712139482105108, 0.1335389361282959, 0.15713484026367722, 0.24845199749997662, 0.28206567058755216, 0.31644458315990853, 0.35136418446315326, 0.6758625033664689, 0.7124216319137535, 0.7490277191169142, 0.7856742013183862, 0.8958064164776166, 0.9325687638529017, 0.9693520243186964, 1.0061539042374907, 0.2222222222222222, 0.2252875011221563, 0.23424278964210216, 0.24845199749997662, 0.31426968052735443, 0.3414646095293662, 0.37037037037037035, 0.40061680838488767, 0.7027283689263065, 0.7379577350063435, 0.7733560376970778, 0.8089010988089465, 0.9162456945817024, 0.9522192690505512, 0.9882714121575062, 1.0243938285880987, 0.25925925925925924, 0.261891400439462, 0.2696336996029821, 0.28206567058755216, 0.3414646095293662, 0.36664796061524685, 0.39370910417535737, 0.4222871944811622, 0.7153039968825173, 0.7499428419006143, 0.7848007444598923, 0.8198497637473576, 0.9259259259259259, 0.9615374063516422, 0.9972527420619453, 1.0330611615541743, 0.2962962962962963, 0.2986021388258722, 0.3054152315272341, 0.31644458315990853, 0.37037037037037035, 0.39370910417535737, 0.41902624070313926, 0.4459849843997146, 0.7295450223552671, 0.7635380788180853, 0.7978021936495563, 0.8323038908979346, 0.9369711585684087, 0.9721781295116065, 1.0075163339804032, 1.0429724326169425, 0.3333333333333333, 0.33538463474583025, 0.3414646095293662, 0.35136418446315326, 0.40061680838488767, 0.4222871944811622, 0.4459849843997146, 0.4714045207910317, 0.7453559924999299, 0.7786591126529125, 0.8122856370170856, 0.8461970117626565, 0.9493337494797257, 0.9840985374508462, 1.0190234438664894, 1.0540925533894598, 0.6666666666666667, 0.6676946806414795, 0.6707692694916606, 0.6758625033664689, 0.7027283689263065, 0.7153039968825173, 0.7295450223552671, 0.7453559924999299, 0.9428090415820635, 0.9693520243186964, 0.9965647442276824, 1.0243938285880987, 1.1111111111111112, 1.140957170425879, 1.171213948210511, 1.2018504251546631, 0.7037037037037037, 0.7046776885348315, 0.7075915990571409, 0.7124216319137535, 0.7379577350063435, 0.7499428419006143, 0.7635380788180853, 0.7786591126529125, 0.9693520243186964, 0.9951873216699558, 1.0217121647506462, 1.0488742413708367, 1.1337205825936272, 1.1629865531931542, 1.192684606526268, 1.222783260682902, 0.7407407407407408, 0.7416660886852143, 0.7444352311941401, 0.7490277191169142, 0.7733560376970778, 0.7848007444598923, 0.7978021936495563, 0.8122856370170856, 0.9965647442276824, 1.0217121647506462, 1.0475656017578483, 1.0740740740740742, 1.1570740260602452, 1.1857637476727498, 1.2149051456930908, 1.2444664901045512, 0.7777777777777778, 0.7786591126529125, 0.7812971522121848, 0.7856742013183862, 0.8089010988089465, 0.8198497637473576, 0.8323038908979346, 0.8461970117626565, 1.0243938285880987, 1.0488742413708367, 1.0740740740740742, 1.0999438818457405, 1.1811273125260722, 1.2092464975788524, 1.2378351827458076, 1.2668615834434866, 0.888888888888889, 0.8896601592195789, 0.8919699687994294, 0.8958064164776166, 0.9162456945817024, 0.9259259259259259, 0.9369711585684087, 0.9493337494797257, 1.1111111111111112, 1.1337205825936272, 1.1570740260602452, 1.1811273125260722, 1.257078722109418, 1.2835350704565522, 1.3105041491077136, 1.337954953199144, 0.9259259259259259, 0.9266663706071706, 0.9288841632581076, 0.9325687638529017, 0.9522192690505512, 0.9615374063516422, 0.9721781295116065, 0.9840985374508462, 1.140957170425879, 1.1629865531931542, 1.1857637476727498, 1.2092464975788524, 1.2835350704565522, 1.3094570021973102, 1.3359028738423284, 1.362842184664748, 0.962962962962963, 0.9636749504635325, 0.9658077637337258, 0.9693520243186964, 0.9882714121575062, 0.9972527420619453, 1.0075163339804032, 1.0190234438664894, 1.171213948210511, 1.192684606526268, 1.2149051456930908, 1.2378351827458076, 1.3105041491077136, 1.3359028738423284, 1.3618352822852027, 1.38827146770306, 1.0, 1.0006856360078737, 1.002739731161547, 1.0061539042374907, 1.0243938285880987, 1.0330611615541743, 1.0429724326169425, 1.0540925533894598, 1.2018504251546631, 1.222783260682902, 1.2444664901045512, 1.2668615834434866, 1.337954953199144, 1.362842184664748, 1.38827146770306, 1.4142135623730951]\n",
            "256\n",
            "256\n",
            "(0, 0.2222222222222222)\n",
            "0.2222222222222222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Higher Dim Case(>2)"
      ],
      "metadata": {
        "id": "1WcZdvI1OPJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "oneDimlist=[]\n",
        "\n",
        "twoDimlist=[]\n",
        "\n",
        "outputlist=[]\n",
        "\n",
        "def increaseDiscreteDim(finalDim):\n",
        "  for i in range(2,finalDim):\n",
        "\n",
        "    \n",
        "\n",
        "    oneDimlist.clear()\n",
        "    twoDimlist.clear()\n",
        "    outputlist.clear()\n",
        "   \n",
        "    HighDim(xaxis,i)\n",
        "    \n",
        "    \n",
        "    z1 = np.array(twoDimlist)\n",
        "    k1 = np.array(outputlist)\n",
        "\n",
        "    #print(twoDimlist)\n",
        "\n",
        "    \n",
        "  \n",
        "    z1_train, z1_test, k1_train, k1_test = train_test_split(z1,k1, test_size=0.2, shuffle=True)\n",
        "\n",
        "    backend.clear_session()\n",
        "    modelh = Sequential()\n",
        "    modelh.add(Dense(3000, input_dim=i, activation='relu')) # Hidden 1  dim increases in each loop\n",
        "    modelh.add(Dense(1500, activation='relu')) # Hidden 2\n",
        "    modelh.add(Dense(1)) # Output\n",
        "    modelh.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    modelh.fit(z1_train,k1_train,verbose=2,epochs=5)\n",
        "\n",
        " \n",
        "\n",
        "increaseDiscreteDim(5)"
      ],
      "metadata": {
        "id": "-607vKclGxHB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cd0a927-e42d-4ad2-fc23-357b32ee8383"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "7/7 - 33s - loss: 0.1976 - 33s/epoch - 5s/step\n",
            "Epoch 2/5\n",
            "7/7 - 0s - loss: 0.0351 - 269ms/epoch - 38ms/step\n",
            "Epoch 3/5\n",
            "7/7 - 0s - loss: 0.0138 - 258ms/epoch - 37ms/step\n",
            "Epoch 4/5\n",
            "7/7 - 0s - loss: 0.0069 - 267ms/epoch - 38ms/step\n",
            "Epoch 5/5\n",
            "7/7 - 0s - loss: 0.0031 - 278ms/epoch - 40ms/step\n",
            "Epoch 1/5\n",
            "103/103 - 4s - loss: 0.0194 - 4s/epoch - 40ms/step\n",
            "Epoch 2/5\n",
            "103/103 - 4s - loss: 1.5540e-05 - 4s/epoch - 37ms/step\n",
            "Epoch 3/5\n",
            "103/103 - 4s - loss: 4.9088e-06 - 4s/epoch - 37ms/step\n",
            "Epoch 4/5\n",
            "103/103 - 4s - loss: 2.3369e-06 - 4s/epoch - 37ms/step\n",
            "Epoch 5/5\n",
            "103/103 - 4s - loss: 1.5168e-06 - 4s/epoch - 37ms/step\n",
            "Epoch 1/5\n",
            "1639/1639 - 61s - loss: 0.0020 - 61s/epoch - 37ms/step\n",
            "Epoch 2/5\n",
            "1639/1639 - 60s - loss: 4.2278e-05 - 60s/epoch - 37ms/step\n",
            "Epoch 3/5\n",
            "1639/1639 - 60s - loss: 1.2551e-04 - 60s/epoch - 37ms/step\n",
            "Epoch 4/5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "x1 = np.array(oneDimlist)\n",
        "y1 = np.array(outputlist)\n",
        "x1_train, x1_test, y1_train, y1_test = train_test_split(x1,y1, test_size=0.2, shuffle=True)\n",
        "x2 = np.array(twoDimlist)\n",
        "y2 = np.array(outputlist)\n",
        "x2_train, x2_test, y2_train, y2_test = train_test_split(x2,y2, test_size=0.2, shuffle=True)\n",
        "\n",
        "#test the 1 dimensional data\n",
        "model1 = Sequential()\n",
        "model1.add(Dense(3000, input_dim=1, activation='relu')) # Hidden 1\n",
        "model1.add(Dense(1500, activation='relu')) # Hidden 2\n",
        "model1.add(Dense(1)) # Output\n",
        "model1.compile(loss='mean_squared_error', optimizer='adam')\n",
        "model1.fit(x1_train,y1_train,verbose=2,epochs=20)"
      ],
      "metadata": {
        "id": "8S0dbmvdS8nh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3eb8db2-38d1-4bc1-b2e7-1a3dffd13324"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "7/7 - 1s - loss: 0.1729 - 625ms/epoch - 89ms/step\n",
            "Epoch 2/20\n",
            "7/7 - 0s - loss: 0.0335 - 260ms/epoch - 37ms/step\n",
            "Epoch 3/20\n",
            "7/7 - 0s - loss: 0.0183 - 266ms/epoch - 38ms/step\n",
            "Epoch 4/20\n",
            "7/7 - 0s - loss: 0.0116 - 270ms/epoch - 39ms/step\n",
            "Epoch 5/20\n",
            "7/7 - 0s - loss: 0.0091 - 250ms/epoch - 36ms/step\n",
            "Epoch 6/20\n",
            "7/7 - 0s - loss: 0.0075 - 255ms/epoch - 36ms/step\n",
            "Epoch 7/20\n",
            "7/7 - 0s - loss: 0.0067 - 253ms/epoch - 36ms/step\n",
            "Epoch 8/20\n",
            "7/7 - 0s - loss: 0.0068 - 424ms/epoch - 61ms/step\n",
            "Epoch 9/20\n",
            "7/7 - 1s - loss: 0.0064 - 640ms/epoch - 91ms/step\n",
            "Epoch 10/20\n",
            "7/7 - 1s - loss: 0.0060 - 553ms/epoch - 79ms/step\n",
            "Epoch 11/20\n",
            "7/7 - 0s - loss: 0.0054 - 350ms/epoch - 50ms/step\n",
            "Epoch 12/20\n",
            "7/7 - 0s - loss: 0.0057 - 272ms/epoch - 39ms/step\n",
            "Epoch 13/20\n",
            "7/7 - 0s - loss: 0.0049 - 267ms/epoch - 38ms/step\n",
            "Epoch 14/20\n",
            "7/7 - 0s - loss: 0.0043 - 260ms/epoch - 37ms/step\n",
            "Epoch 15/20\n",
            "7/7 - 0s - loss: 0.0043 - 251ms/epoch - 36ms/step\n",
            "Epoch 16/20\n",
            "7/7 - 0s - loss: 0.0036 - 268ms/epoch - 38ms/step\n",
            "Epoch 17/20\n",
            "7/7 - 0s - loss: 0.0036 - 249ms/epoch - 36ms/step\n",
            "Epoch 18/20\n",
            "7/7 - 0s - loss: 0.0040 - 253ms/epoch - 36ms/step\n",
            "Epoch 19/20\n",
            "7/7 - 0s - loss: 0.0037 - 261ms/epoch - 37ms/step\n",
            "Epoch 20/20\n",
            "7/7 - 0s - loss: 0.0036 - 263ms/epoch - 38ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5793278d90>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test the >2 dimensional data\n",
        "model2 = Sequential()\n",
        "model2.add(Dense(3000, input_dim=2, activation='relu')) # Hidden 1\n",
        "model2.add(Dense(1500, activation='relu')) # Hidden 2\n",
        "model2.add(Dense(1)) # Output\n",
        "model2.compile(loss='mean_squared_error', optimizer='adam')\n",
        "model2.fit(x2_train,y2_train,verbose=2,epochs=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0MolQ5cNEwM",
        "outputId": "f5223098-027c-46d8-ce48-a40a627a8959"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "7/7 - 1s - loss: 0.2048 - 602ms/epoch - 86ms/step\n",
            "Epoch 2/20\n",
            "7/7 - 0s - loss: 0.0313 - 262ms/epoch - 37ms/step\n",
            "Epoch 3/20\n",
            "7/7 - 0s - loss: 0.0103 - 279ms/epoch - 40ms/step\n",
            "Epoch 4/20\n",
            "7/7 - 0s - loss: 0.0045 - 255ms/epoch - 36ms/step\n",
            "Epoch 5/20\n",
            "7/7 - 0s - loss: 0.0019 - 247ms/epoch - 35ms/step\n",
            "Epoch 6/20\n",
            "7/7 - 0s - loss: 0.0011 - 257ms/epoch - 37ms/step\n",
            "Epoch 7/20\n",
            "7/7 - 0s - loss: 5.8483e-04 - 255ms/epoch - 36ms/step\n",
            "Epoch 8/20\n",
            "7/7 - 0s - loss: 3.1888e-04 - 264ms/epoch - 38ms/step\n",
            "Epoch 9/20\n",
            "7/7 - 0s - loss: 2.2889e-04 - 249ms/epoch - 36ms/step\n",
            "Epoch 10/20\n",
            "7/7 - 0s - loss: 1.3664e-04 - 256ms/epoch - 37ms/step\n",
            "Epoch 11/20\n",
            "7/7 - 0s - loss: 9.3762e-05 - 258ms/epoch - 37ms/step\n",
            "Epoch 12/20\n",
            "7/7 - 0s - loss: 5.5664e-05 - 274ms/epoch - 39ms/step\n",
            "Epoch 13/20\n",
            "7/7 - 0s - loss: 3.6246e-05 - 250ms/epoch - 36ms/step\n",
            "Epoch 14/20\n",
            "7/7 - 0s - loss: 3.1154e-05 - 261ms/epoch - 37ms/step\n",
            "Epoch 15/20\n",
            "7/7 - 0s - loss: 2.6582e-05 - 270ms/epoch - 39ms/step\n",
            "Epoch 16/20\n",
            "7/7 - 0s - loss: 2.2296e-05 - 267ms/epoch - 38ms/step\n",
            "Epoch 17/20\n",
            "7/7 - 0s - loss: 1.9817e-05 - 256ms/epoch - 37ms/step\n",
            "Epoch 18/20\n",
            "7/7 - 0s - loss: 1.7466e-05 - 259ms/epoch - 37ms/step\n",
            "Epoch 19/20\n",
            "7/7 - 0s - loss: 1.6344e-05 - 254ms/epoch - 36ms/step\n",
            "Epoch 20/20\n",
            "7/7 - 0s - loss: 1.3747e-05 - 257ms/epoch - 37ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f57931ec1d0>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    }
  ]
}
